{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Sheet 3 - Covid-19 Classification using Transfer Learning\n",
    "\n",
    " * Deep Learning for Computer Vision â€“ Winter Term 2025/26\n",
    " * Lecture by: Constantin Pape\n",
    " * Organizers: Anwai Archit, Sushmita Nair\n",
    " * Tutors: Azhar Akhmetova, Benjamin Eckhardt, Carolin Teuber, Luca Freckmann, Marei Freitag, Oleg Bakumenko, Sarah Muth\n",
    " * Due date: **Tuesday, Dec 9, before 10:00**\n",
    "## Time Required to Solve this Exercise Sheet\n",
    "\n",
    "As you will train deep CNNs on this exercise sheet, model training will require an increased amount of time. So we recommend to start working on this sheet early.\n",
    "\n",
    "## Topic\n",
    "\n",
    "In this exercise, you will solve an image classification task from medical imaging: classification in Chest X-Ray images into patients with Covid-19, Pneunomia or Healthy. We will use a subset of the dataset from a Kaggle challenge for this. (https://www.kaggle.com/datasets/prashant268/chest-xray-covid19-pneumonia)\n",
    "\n",
    "The main focus of this exercise is transfer learning and you will approach the classification task with three different approaches:\n",
    "\n",
    "    Training ResNets from scratch. (Note that we will use the ResNet implementation from torchvision throughout the exercise).\n",
    "    Training ResNets pretrained on ImageNet.\n",
    "    Training ResNets pretrained on RadImageNet, a large radiology dataset.\n",
    "\n",
    "In addition you can combine these approaches with other methods to improve the model at the end and upload your best solution on a hold-out test set. This is explained in more details at the end of the exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hints\n",
    "\n",
    "In the first part of the exercise you will train ResNets from scratch, analyze the effects of deeper models with small training data and use data augmentations. In the second part, you will solve the same task using pretrained ResNets (from ImageNet - pretrained on natural images; and RadImageNet - pretrained on the medical imaging domain).\n",
    "\n",
    "To understand the background of this exercise, you can:\n",
    "- Review the lectures\n",
    "    - Lecture 3 on CNNs\n",
    "    - Lecture 4 on Transfer Learning and Augmentaion\n",
    "- Check out the [RadImageNet publication](https://doi.org/10.1148/ryai.210315).\n",
    "\n",
    "At the end of the exercise you should further improve your model. You can draw upon a number of techniques we discussed for improving model performance. The predictions from your best model on a hold-out test set should be uploaded together with the exercise. More explanation is given at the end of the exercise sheet.\n",
    "\n",
    "_Do not hesitate to ask questions and ideally discuss them with the fellow students on Matrix! We will monitor the channel to provide you help if your discussions get stuck._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTANT SUBMISSION INSTRUCTIONS\n",
    "- **You need to answer all the questions in written form**\n",
    "- When you are done, download the notebook and **rename** it to `<surname1>_<surname2>_<surname3>.ipynb`\n",
    "- For the final submission:\n",
    "    - Submit the **Jupyter Notebook** (.ipynb file). Upload them on `Stud.IP` -> `Deep Learning for Computer Vision` -> `Files` -> `Submission for Homework 3` -> `Notebook` -> `Tutorial <X>` (where, `X` is the tutorial you are assigned to).\n",
    "    - Submit the **Challenge Results** (.csv file) (for the unlabeled images, namely `unknown`). Upload them on `Stud.IP` -> `Deep Learning for Computer Vision` -> `Files` -> `Submission for Homework 3` -> `Challenge Results` -> `Tutorial <X>`\n",
    "- Make only one submission of the exercise and results per group.\n",
    "- The deadline is strict.\n",
    "- You have to present the exercise in the tutorials. We have a high ratio of students to tutors, so please decide which team member presents which part beforehand.\n",
    "\n",
    "Implementation\n",
    "- Do not change the cells which are marked as `DO NOT CHANGE`, similarly write your solution into the cells marked with TODOs and answer the **questions** asked. In addition to the python packages loaded below, you are allowed to use any packages you want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing required libraries:**\n",
    "\n",
    "We have provided the most essential libraries to be used for the exercise. Feel free to add modules as per your requirement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from IPython.display import FileLink\n",
    "\n",
    "import numpy as np\n",
    "import imageio.v3 as imageio\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "%pip install barbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the device (make sure device returns \"cuda\" to use of the GPUs)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the Required Functionalities\n",
    "\n",
    "We first download and import `ex3_utils.py`, which in which functions for training evaluation etc. are already implemented (similar to the previous exercise). You can download this file and inspect it on your computer to understand the functions it contains.\n",
    "\n",
    "You can download it from Stud.IP (it's provided there in the same folder as this notebook.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE\n",
    "import ex3_utils as utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "The dataset is provided for you in the Project Folder. If you're working on your local machines you can download it as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a directory to save checkpoints, feel free to change this\n",
    "root_dir = \"./\"\n",
    "\n",
    "# DO NOT CHANGE\n",
    "# Check if $PROJECT exists\n",
    "project_path = os.path.expandvars(\"$PROJECT\")\n",
    "\n",
    "if os.path.exists(project_path):\n",
    "    # On cluster: use $PROJECT paths\n",
    "    data_folder = os.path.join(project_path, \"data\", \"Ex3\", \"covid19\")\n",
    "    model_dir = os.path.join(project_path, \"models\", \"Ex3\", \"model_weights\")\n",
    "else:\n",
    "    # Local machine\n",
    "    data_folder = os.path.join(root_dir, \"covid19\")\n",
    "    model_dir = os.path.join(root_dir, \"model_weights\")\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    !wget https://owncloud.gwdg.de/index.php/s/dcvhmxtksDDDtK8/download -O covid19-xray.zip \n",
    "    !unzip -q \"covid19-xray.zip\" -d {data_folder}\n",
    "\n",
    "print(f\"Data folder: {data_folder}\")\n",
    "print(f\"Model weights folder: {model_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data exploration**\n",
    "\n",
    "The data is stored in the project folder (or root_dir) and contains four subfolders, namely `train`, `val`, `test` and `unknown` with training / validation / testing split and the hold-out test set, respectively. Each of these (except `unknown`) contains subfolders with the images for the respective classes. As a first step, we will visualize some of the images and labels from the training data.\n",
    "\n",
    "We will not use the `unknown` directory for training or evaluation, but will only use it for prediction with the best model at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE\n",
    "# Checking the number of classes\n",
    "train_class_dirs = glob(os.path.join(data_folder, \"train\", \"*\"))\n",
    "val_class_dirs = glob(os.path.join(data_folder, \"val\", \"*\"))\n",
    "test_class_dirs = glob(os.path.join(data_folder, \"test\", \"*\"))\n",
    "assert len(train_class_dirs) == len(val_class_dirs) == len(test_class_dirs) == 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE\n",
    "# Checking the expected structure of all images\n",
    "image_paths = glob(os.path.join(data_folder, \"**\", \"*.jpg\"), recursive=True)\n",
    "assert len(image_paths) == (450 + 150 + 225 + 5607), len(image_paths)  # 450 train samples, 150 val samples, 225 test samples, 5607 unlabeled samples (hold-out test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot samples from each class in the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classes available for the task\n",
    "classes = [\"COVID19\", \"NORMAL\", \"PNEUMONIA\"]\n",
    "num_classes = 3\n",
    "\n",
    "# TODO: YOUR SOLUTION HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see x-ray images from each class and their corresponding labels in their titles. To reflect on our understanding of the dataset, please answer the following **questions**:\n",
    "- How many classes are provided in the datasets?\n",
    "- Are the images balanced along all the classes in the respective data splits?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set the directories for the respective datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits for the dataset \n",
    "train_dir = os.path.join(data_folder, \"train\")\n",
    "val_dir = os.path.join(data_folder, \"val\")\n",
    "test_dir = os.path.join(data_folder, \"test\")\n",
    "unknown_dir = os.path.join(data_folder, \"unknown\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we obtain the statistics (mean and standard deviation) from the training dataset to be used for normalizing the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE\n",
    "train_images = []\n",
    "for class_name in classes:\n",
    "    train_images.extend(glob(os.path.join(train_dir, class_name, \"*.jpg\")))\n",
    "\n",
    "to_tensor = torchvision.transforms.ToTensor()\n",
    "imgs = torch.stack([to_tensor(imageio.imread(im)) for im in train_images])\n",
    "print(imgs.shape)\n",
    "\n",
    "# And then compute the mean and standard deviation independently for the image channels.\n",
    "# (The channels are stored in dim=1, by excluding this below we achieve this.)\n",
    "mean = torch.mean(imgs, dim=(0, 2, 3))\n",
    "std = torch.std(imgs, dim=(0, 2, 3))\n",
    "\n",
    "# Delete the images again in order to save memory.\n",
    "del imgs\n",
    "\n",
    "print(mean)\n",
    "print(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_mean = mean\n",
    "train_dataset_std = std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implement PyTorch dataloader**\n",
    "\n",
    "As a next step, we implement a `torch.utils.data.Dataset` followed by the `torch.utils.data.DataLoader` to have access to our data during training, validation and testing. In our case, the data is stored in a format that is already compatible with `torchvision.datasets.ImageFolder`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the image data in PyTorch it first needs to be transformed. You can use the transformations from [torchvision.transforms](https://pytorch.org/vision/0.9/transforms.html) for this. Here, we need to:\n",
    "- Convert the image data to a `torch.tensor` (`transforms.ToTensor`)\n",
    "- Standardize the inputs based on their data statistics (`transforms.Normalize`)\n",
    "- Resize the images (`transforms.Resize`). Note that resizing is not strictly necessary, but will speed up training and resize the images to a size that better matches the ImageNet pretraining data.\n",
    "\n",
    "To combine several transforms together, you can use `torchvision.transforms.Compose` by passing the list of 'transform' objects to compose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We resize the images to the size 256 x 256 to speed up training\n",
    "height = width = 256\n",
    "\n",
    "# Get the usual transforms to have the inputs from dataloaders as expected\n",
    "def get_transforms(height, width):\n",
    "    # TODO: YOUR CODE HERE\n",
    "    transform = ...\n",
    "    return transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size\n",
    "batch_size = ...  # TODO: YOUR CHOICE HERE\n",
    "\n",
    "# Datasets\n",
    "# TODO: YOUR SOLUTION HERE\n",
    "train_dataset = ...\n",
    "val_dataset = ...\n",
    "test_dataset = ...\n",
    "\n",
    "# Dataloaders\n",
    "# TODO: YOUR SOLUTION HERE\n",
    "train_loader = ...\n",
    "val_loader = ...\n",
    "test_loader = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's visualize the images after transformation as they are returned from the training loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to show images\n",
    "fig = plt.figure(figsize=(10, 15))\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5  # unnormalise\n",
    "    npimg = img.numpy()\n",
    "    plt.axis('off')\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "# Obtain random training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# Show the images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "\n",
    "print(' '.join('%5s, ' % classes[labels[j]] for j in range(len(labels))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "\n",
    "We will use the ResNet implementation from torchvision, see https://pytorch.org/vision/stable/models.html, for this exercise.\n",
    "\n",
    "- We start with the smallest ResNet model, the ResNet18 (https://pytorch.org/vision/main/models/generated/torchvision.models.resnet18.html)\n",
    "- And will then use deeper ResNets to explore how well larger models can be trained on a small dataset.\n",
    "- To use random weight initialization (in order to train from scratch), you can just create models without passing additional arguments like so: `torchvision.models.<MODEL_NAME>()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to adapt the model to fit with our purpose. For this, we must change the output dimension of the last fully-connected layer to consider the number of classes in our problem. Let's check the last layer out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get the resnet18 architecture and understanding the backbone (in order to adapt it to our problem)\n",
    "# TODO: YOUR SOLUTION HERE\n",
    "model = ...\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see above that there is a component called `fc` that consists of a `Linear` layer. To make use of ResNet18 for our classification task, we need to change the `out_features` of the linear layer to the number of classes of our problem. In our case, it's `num_classes` (=3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.fc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To replace the last layer (classification layer) you can use the following code. (Here we provide a stand-alone code snippet)\n",
    "\n",
    "```python\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "\n",
    "model = torchvision.models.resnet18()\n",
    "\n",
    "# Let's replace the \"fully connected\" layer to match our expected output classes\n",
    "model.fc = nn.Linear(<INPUT_FEATURES>, <OUTPUT_CLASSES>)\n",
    "model.to(device)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing the last layer(s) of ResNet18 to match our number of classes\n",
    "# TODO: YOUR SOLUTION HERE\n",
    "model = ...\n",
    "\n",
    "\n",
    "# Let's define the checkpoint name where the specific model checkpoint will be saved\n",
    "model_name = ...\n",
    "checkpoint_name = f'covid-19-{model_name}-from-scratch.pt'\n",
    "checkpoint_path = os.path.join(root_dir, checkpoint_name)\n",
    "print(\"The model checkpoint will be saved here: \", checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you now display the model (using `print(model)`) you should see that its last layer has been updated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Let's train the model for 10 epochs, using the `Adam` optimizer, `CrossEntropyLoss` as the criterion (loss function) and a learning rate scheduler (e.g. `ReduceLROnPlateau`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clearing the GPU cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "print(\"Starting training from scratch with network: \", model_name)\n",
    "\n",
    "\n",
    "# TODO: YOUR SOLUTION HERE\n",
    "# Optimizer and loss configurations\n",
    "criterion = ...  # loss function\n",
    "optimizer = ...  # optimizer\n",
    "scheduler = ...  # learning rate scheduler\n",
    "\n",
    "\n",
    "# Initializing the early stopping of the training\n",
    "early_stopping = utils.EarlyStopping(...)\n",
    "\n",
    "\n",
    "# HINT: open the 'ex3_utils.py' file to understand the arguments of 'run_training'\n",
    "... = utils.run_training(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see how the accuracy and loss plots look for our training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and validation accuracy plots\n",
    "# TODO: YOUR SOLUTION HERE\n",
    "utils.get_metric_plots(...)\n",
    "\n",
    "\n",
    "# Training and validation loss plots\n",
    "# TODO: YOUR SOLUTION HERE\n",
    "utils.get_metric_plots(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation on the test set\n",
    "\n",
    "Let's check the test accuracy and confusion matrix. All the required functionality is already provided in `ex3_utils.py` and you just need to read the corresponding functions to understand how to call them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the best model for inference (NOTE: you should load the checkpoints to the expected model architecture, else you might get some mismatch errors)\n",
    "# TODO: YOUR SOLUTION HERE\n",
    "\n",
    "\n",
    "# Provide the testing dataset wrapped in a dataloader to check for inference\n",
    "# TODO: YOUR SOLUTION HERE\n",
    "correct, total = utils.test_evaluation(...)\n",
    "print(f'Accuracy of {checkpoint_path} on the Test Images: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the confusion matrix for the test dataset\n",
    "# TODO: YOUR SOLUTION HERE\n",
    "cm = utils.get_confusion_matrix(...)\n",
    "\n",
    "\n",
    "# See the precision, recall and accuracy per class for the test dataset\n",
    "# TODO: YOUR SOLUTION HERE\n",
    "utils.check_precision_recall_accuracy(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the confusion matrix heatmap to visually see the evaluation on the test set\n",
    "# TODO: YOUR SOLUTION HERE\n",
    "utils.visualize_confusion_matrix(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try training deeper ResNet architectures using the same hyperparameters from above:\n",
    "- Train and evaluate a ResNet34 from scratch.\n",
    "- Train and evaluate a ResNet50 from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: YOUR SOLUTIONS HERE\n",
    "\n",
    "\n",
    "# HINT: (for the workflow)\n",
    "#    - Start with training a ResNet34:\n",
    "#          - Updating the last layers (training the network from scratch)\n",
    "#          - REMEMBER: Save the respective checkpoints uniquely, and initialize the early stopping with them.\n",
    "#          - Use the expected hyperparameters for training\n",
    "#          - Observe the loss and accuracy curves for training and validation\n",
    "#    - Next, train a ResNet50 (following the same suggestion as above)\n",
    "#    - Finally, evaluate both the trained models from above on the test set and answer the questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**:\n",
    "\n",
    "1. How does the performance of the three architectures compare to each other? Are there any specific patterns you can see in the confusion matrices?\n",
    "2. Is there a correlation between the dataset size and depth of the network?\n",
    "3. Given these observations, which model would you prefer for training on a small dataset from scratch?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's try with **Data Augmentation**\n",
    "\n",
    "A common strategy when dealing with small datasets is to add data augmentations.\n",
    "\n",
    "Let's try a few augmentations from [torchvision.transforms](https://pytorch.org/vision/main/transforms.html) that do not significantly distort the data (stay on the data manifold) and train a ResNet50. Here, we want to see if augmentations allow us to train deeper CNNs. \n",
    "Common augmentations for data augmentation are:\n",
    "- flipping the inputs along the axes.\n",
    "- changing the brightness, contrast and saturation of the inputs.\n",
    "- adding small noise to the input.\n",
    "\n",
    "You can try different combinations of augmentations here. You can add augmentations by updating the `transform` passed to the train dataset. You can again use `Compose` to chain augmentations. Play around with different augmentations here but if you can not find any data augmentations that improve model performance, move on to the next task. \n",
    "\n",
    "Note; you should only introduce augmentation for the training and validation datasets, **not** for the test dataset. Otherwise the evaluation between models is not consistent anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: YOUR SOLUTIONS HERE\n",
    "\n",
    "# HINT:\n",
    "#    - Training a ResNet50 (incorporating data augmentation strategies)\n",
    "#    = (the training workflows are the same as above)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to apply the new transforms to the dataset and then use our dataloader. Let's reuse our scripts from above, now with the added data augmentation strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: YOUR SOLUTION HERE\n",
    "# Datasets\n",
    "train_dataset = ...\n",
    "val_dataset = ...\n",
    "\n",
    "# TODO: YOUR SOLUTION HERE\n",
    "# Dataloaders\n",
    "train_loader = ...\n",
    "val_dataset = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are ready to train the ResNet50 with data augmentations. Let's train for longer (30 epochs) here, with the same hyperparameters as before, and evaluate the model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: YOUR SOLUTION HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**:\n",
    "\n",
    "1. Does the effect of data augmentation match your expectation?\n",
    "2. Are there other kinds of data augmentation that would make sense for this dataset? Think about augmentations that \"leave the data manifold\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ImageNet\n",
    "\n",
    "The [ImageNet project](https://www.image-net.org/) provides a large-scale dataset with natural images. There exist different version of this dataset, the largest using 14 million annotated images for image classification with over 20,000 categories. \n",
    "\n",
    "This dataset has been used by the ImageNet Scale Visual Recognition Challenge ([ILSVRC](https://image-net.org/challenges/LSVRC/#:~:text=The%20ImageNet%20Large%20Scale%20Visual,image%20classification%20at%20large%20scale.)) to benchmark classification algorithms competing to improve classification. The version fo the dataset used for this classification contains a million training images with 1,000 categeories (and corresponds to the version fo the dataset we discussed in the lecture).\n",
    "\n",
    "`torchvision.models` has a pool of neural networks, for which pretrained ImageNet weights are also available. To make use of pretrained models, we need to preprocess the images based on the mean and standard deviation of ImageNet (this step is critical!). The statistics for ImageNet are provided [here](https://pytorch.org/vision/main/models/generated/torchvision.models.resnet50.html#:~:text=1.0%5D%20and%20then-,normalized%20using%20mean%3D%5B0.485%2C%200.456%2C%200.406%5D%20and%20std%3D%5B0.229%2C%200.224%2C%200.225%5D.,-Next).\n",
    "\n",
    "### ImageNet transfer learning for ResNet50\n",
    "\n",
    "We will now check if transfer learning from ImageNet leads to improvements for our dataset. Here, we will focus on the ResNet50 as we have seen before that without transfer learning its performance is worse compared to smaller architectures for our dataset.\n",
    "\n",
    "There are two different approaches for how we can fine-tune a CNN initialized with pretrained weights:\n",
    "1. Update the weights for all layers of the network. For this approach we initialize with pretrained weights but otherwise train the network as before. See this code snippet:\n",
    "\n",
    "    ```python\n",
    "    model_dir = \"./model_weights\"\n",
    "    weights_path = os.path.join(model_dir, \"resnet50-0676ba61.pth\")\n",
    "\n",
    "    model = torchvision.models.resnet50(weights=None)  # or pretrained=False in older torchvision\n",
    "\n",
    "    # Load the state dictionary\n",
    "    state_dict = torch.load(weights_path, map_location=device)\n",
    "    model.load_state_dict(state_dict)\n",
    "    # Let's replace the \"fully connected\" layer to match our expected output classes\n",
    "    model.fc = nn.Linear(<INPUT_FEATURES>, <OUTPUT_CLASSES>)\n",
    "    model.to(device)\n",
    "\n",
    "    # Train the network as usual\n",
    "    ```\n",
    "\n",
    "    or if you are not using Jupyter HPC: \n",
    "\n",
    "    ```python\n",
    "    import torchvision\n",
    "    import torch.nn as nn\n",
    "\n",
    "    model = torchvision.models.resnet50(weights=torchvision.models.ResNet50_Weights.DEFAULT)\n",
    "    # Let's replace the \"fully connected\" layer to match our expected output classes\n",
    "    model.fc = nn.Linear(<INPUT_FEATURES>, <OUTPUT_CLASSES>)\n",
    "    model.to(device)\n",
    "\n",
    "    # Train the network as usual\n",
    "    ```\n",
    "    \n",
    "2. Only update the weights of the last layer (classification layer). In this case we would \"freeze\" the pretrained network and use it as a fixed feature extractor. This is achieved by disabling parameter updates for all but the last layer of the network.\n",
    "\n",
    "We will start with the first approach (fine-tuning the full network)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: YOUR SOLUTIONS HERE\n",
    "\n",
    "# HINT:\n",
    "#    - Training a ResNet50 (updating all the layers, transfer learning using ImageNet weights)\n",
    "#    = (the training workflows are the same as above)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try the second approach and finetune only the last layer.\n",
    "\n",
    "By default all parameters of a network will be updated by gradient descent. The gradient updates can be disabled by setting `requires_grad = False`. So in order to finetune only the last layer you need to `requires_grad = False` for all other layers in the network.\n",
    "\n",
    "You can see how the parameters are disabled for a complete model in the code snippet below. For the next exercise you need to make sure that `requires_grad` stays `True` for the last layer!\n",
    "\n",
    "```python\n",
    "for param_name, param in model.named_parameters():\n",
    "    param.requires_grad = False\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: YOUR SOLUTIONS HERE\n",
    "\n",
    "# HINT:\n",
    "#    - Training a ResNet50 (updating the last layers, transfer learning using ImageNet weights)\n",
    "#    = (the training workflows are the same as above)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**:\n",
    "\n",
    "1. Elaborate on the reason to use pretrained ImageNet weights for transfer learning (instead of training from scratch).\n",
    "2. Which transfer learning approach performs better for finetuning on our dataset? Comment on the possible reasons!\n",
    "3. Explain the best use-cases for ImageNet pretrained weights for the two different approaches (i.e. when it is a good idea to train from scratch OR for finetuning all layers OR for finetuning last layers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment:** When finetuning a model it may also be beneficial to use a different learning rate and change other hyperparameters compared to training from scratch. To keep the exercise simple we do not further explore these options here. If you're interested to investigate these effects you can explore different hyperparameters at the end of the exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RadImageNet\n",
    "\n",
    "A disadvantage of using ImageNet pretraining for medical images is that the pretraining data is very different to medical images. There are several efforts to build pretraining datasets for the medical image domain, for example [RadImageNet](https://www.radimagenet.com/).\n",
    "It contains a pretraining dataset made up of radiology images of a comparable size to ImageNet and provides networks that were pretrained with this dataset.\n",
    "- For more details you can check out the [RadImageNet publication](https://doi.org/10.1148/ryai.210315).\n",
    "\n",
    "Note: if you use on RadImageNet weights you will have to use the statistics below for normalization (corresponding to the image statistics of the RadImageNet dataset):\n",
    "```python\n",
    "# Normalization: mean and standard deviation values for the pretrained weights on RadImageNet dataset\n",
    "radimagenet_mean = (0.223, 0.223, 0.223)\n",
    "radimagenet_std = (0.203, 0.203, 0.203)\n",
    "```\n",
    "\n",
    "We now use pretrained RadImageNet weights for the ResNet50. For this we first need to get the weights (they were previously downloaded for you) and initialize the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE\n",
    "radimagenet_checkpoint_path = os.path.join(model_dir, \"RadImageNet_pytorch\")\n",
    "\n",
    "resnet50_ckpt = torch.load(os.path.join(radimagenet_checkpoint_path, \"ResNet50.pt\"), weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE\n",
    "# We create the backbone to intialize it with the pretrained weights from RadImageNet\n",
    "class Backbone(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        base_model = torchvision.models.resnet50()\n",
    "        encoder_layers = list(base_model.children())\n",
    "        self.backbone = nn.Sequential(*encoder_layers[:9])\n",
    "                        \n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "\n",
    "# We build the classifier to use the features for transfer learning\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, num_class):\n",
    "        super().__init__()\n",
    "        self.drop_out = nn.Dropout()\n",
    "        self.linear = nn.Linear(2048, num_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.drop_out(x)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now combine the backbone (ResNet50 pretrained on RadImageNet) and the classification layer for our dataset.\n",
    "\n",
    "Train and evaluate this model. You can decide whether to finetune the whole model or just the last layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's call the backbone (the encoder excluding the last layers)\n",
    "backbone = Backbone()\n",
    "\n",
    "# Loading the pretrained weights to the backbone\n",
    "backbone.load_state_dict(resnet50_ckpt)\n",
    "\n",
    "# Now let's call the expected fully connected layer\n",
    "classifier = Classifier(num_class=len(classes))\n",
    "\n",
    "# Finally, we are ready to build our model \n",
    "net = nn.Sequential(backbone, classifier).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: YOUR SOLUTIONS HERE\n",
    "\n",
    "# HINT:\n",
    "#    - Training a ResNet50 (updating all / last layers, transfer learning using RadImageNet weights)\n",
    "#    = (the training workflows are the same as above)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**:\n",
    "\n",
    "1. Comment on your choice of transfer learning approach (either finetuning last layers / all layers in the model) for RadImageNet? Why could the best approach here be different from ImageNet?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall Results:\n",
    "\n",
    "Update the table with your results for all experiments from the exercise:\n",
    "\n",
    "| Model    | Training                         | Test Accuracy |\n",
    "|:-------: |:--------------------------------:|:-------------:|\n",
    "| ResNet18 | from scratch                     |               |\n",
    "| ResNet34 | from scratch                     |               |\n",
    "| ResNet50 | from scratch                     |               |\n",
    "| ResNet50 | from scratch (with augmentation) |               |\n",
    "| ResNet50 | from ImageNet (all layers)       |               |\n",
    "| ResNet50 | from ImageNet (last layers)      |               |\n",
    "| ResNet50 | from RadImageNet                 |               |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**:\n",
    "1. Comment on the trend observed in the results from the table.\n",
    "2. Which is the overall best model? Does this match your expectations? Why/why not?\n",
    "\n",
    "**Important: Please read the end of the exercise sheet and upload the predictions from your best model, also if you choose not to try out to further improve it.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further improve your model (*Optional*)\n",
    "\n",
    "You can now try to further improve the model by using different architectures or trying some of the advanced training and inference techniques discussed in the lecture. This part of the exercise is optional, and you can try to apply as many approaches for improving your method as you would like. If you don't want to work on the optional part at all please go to the end of the exercise to submit the predictions from your best model.\n",
    "\n",
    "Here are the aproaches you can try to improve the model:\n",
    "1. Update the training hyperparameter:\n",
    "    - You can choose a better initial learning rate, or choose different options for the learning rate scheduler or early stopping.\n",
    "    - The most important parameter is probably the learning rate; if you want to improve it then train only for a short time and compare different values. (Remember lecture 2 and the first exercise). \n",
    "2. Try other architectures provided in `torchvision.models`, which implements further CNN architectures (and their respective ImageNet pretrained weights). For example `DenseNet` or `EfficientNet` could yield better results than ResNets.\n",
    "    - `torchvision.models` also offers vision transformer architectures (https://arxiv.org/abs/2010.11929). We will cover those later in the lecture, but if you want you can try them here as well. See the snippet at the end of this section for details. \n",
    "3. Try advanced data augmentation strategies, which change the data manifold severly and can boost performance. For example MixUp or CutMix. `torchivsion` already implements them, see [data augmentation with torchvision](https://pytorch.org/vision/stable/transforms.html) for details.\n",
    "4. Try test-time data augmentation. We have covered this idea in the lecture. To use it you don't need to change the training at all, but just update the model prediction during testing. Fot this you can either update the functionality in `ex3_utils.py` or implement a wrapper function or class around the model that implements the test-time data augmentation logic.\n",
    "5. Try model ensembling: combine the predictions of different models on the test set or use an implicit averaging approach like polyak averaging. For this approaches you may need to update the training and/or prediction functions from `ex3_utils.py`.\n",
    "\n",
    "Here's a snippet that shows how to use vision transformers from torchvision:\n",
    "\n",
    "```python\n",
    "# replacing the respective layers of the \"transformer-based networks\" to match our number of classes\n",
    "net = torchvision.models.vit_b_16(pretrained=True)\n",
    "net.heads.head = nn.Linear(768, num_classes)\n",
    "net.to(device)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint**: if you want to systematically improve your model you should proceed as follows:\n",
    "- Decide on the base architecture and how to initialize it.\n",
    "    - Either choose the best model you according to the exercise so far or try if advanced architectures from `torchvision` bring an improvement and continue with one of them.\n",
    "- Optimize the training hyperparameters (learning rate, scheduling, early stopping).\n",
    "- Investigate advanced techniques:\n",
    "  - Training with more or advanced data augmentation.\n",
    "  - Test-time data augmentation\n",
    "  - Model ensembling\n",
    "\n",
    "You can divide some of these tasks up among your group and train the best model by combining the best settings you have found for the individual steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train your best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: IMPLEMENT AND TRAIN THE BEST MODEL HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission:\n",
    "\n",
    "- The `unknown` set corresponds to a hold-out test set (with unlabeled images). Such unseen test data is common in machine learning challenges to ensure an objective comparison of different methods and to test how well these solutions would generalize to real data.\n",
    "- Submit the predictons for the `unknown` set using your best model together with your exercise solution.\n",
    "    - Upload the solutions to `Stud.IP` -> `Deep Learning for Computer Vision` -> `Files` -> `Submission for Homework 3` -> `Challenge Results`-> `Tutorial <X>`.\n",
    "    - Your submission should be called `results_surname1_surname2_surname3.csv`. The expected file format is described below and we provide a function that generates these results for you already.\n",
    "- **Please submit the results from your best model (or the model you expect to perform best). If you don't work on the optional part of the exercise then submit the best result from the models you have trained so far.**\n",
    "    - The group with the best submission will get a small prize ;-) \n",
    "\n",
    "### Expected Submission Format\n",
    "\n",
    "Before submitting your results, please make sure that they are in the below mentioned format:\n",
    "- `results_<surname1>_<surname2>_<surname3>.csv`\n",
    "    - patient_\\<ID-1>.jpg | \\<CLASS-1>\n",
    "    - patient_\\<ID-2>.jpg | \\<CLASS-2> <br>\n",
    "    . <br>\n",
    "    . <br>\n",
    "    . <br>\n",
    "    - patient_\\<ID_n>.jpg | \\<CLASS-[1-3]>\n",
    " \n",
    "We have prepared a function that generates these predictions for you (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: YOUR SOLUTION HERE\n",
    "filename = \"results.csv\"\n",
    "\n",
    "# Function generating predictions (in a csv file) for the respective images in the hold-out test set (`unknown` folder)\n",
    "# Note: if you want to use test-time-augmentation or model ensembling you may need to update this function.\n",
    "# If you don't want to update it you can also create a new class that implements a wrapper around the model(s)\n",
    "# that implements the augmentation or ensembling logic.\n",
    "utils.predict_unknown(net, height, width, train_dataset_mean, train_dataset_std, unknown_dir, device, filename)\n",
    "\n",
    "# Download link is automatically generated for the final results generated\n",
    "FileLink(filename)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
